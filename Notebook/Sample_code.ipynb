{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages & Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "def get_pipeline(path:str, tokenizer:AutoTokenizer, accelerator:accelerate.Accelerator) -> TextGenerationPipeline:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path, torch_dtype=torch.float16, device_map='auto', trust_remote_code=True)\n",
    "    \n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.pad_token_id]\n",
    "\n",
    "    pipeline = TextGenerationPipeline(model = model, tokenizer = tokenizer, num_workers=accelerator.state.num_processes*4, pad_token_id=tokenizer.pad_token_id, eos_token_id=terminators)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taigi-Llama-2-series: Causal Language Modeling for Taigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a2bae2f773423499b260c4d2d37b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = \"Bohanlu/Taigi-Llama-2-7B\" # or Bohanlu/Taigi-Llama-2-13B for the 13B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "\n",
    "accelerator = accelerate.Accelerator()\n",
    "pipe = get_pipeline(model_dir, tokenizer, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CausalLM示例：台語文本生成\n",
    "clm_prompt = \"我真愛食台灣的\"\n",
    "\n",
    "# Few-shot示例：情感分析\n",
    "sentiment_prompt = \"\"\"Example 1:\n",
    "輸入：這齣電影真正是蓋讚啦！\n",
    "情感：正面\n",
    "\n",
    "Example 2:\n",
    "輸入：今仔日的天氣真䆀。\n",
    "情感：負面\n",
    "\n",
    "Example 3:\n",
    "輸入：這間餐廳的服務一般般仔爾爾。\n",
    "情感：中性\n",
    "\n",
    "Example 4:\n",
    "輸入：我拄買彼支手機仔的螢幕誠大塊。\n",
    "情感：\"\"\"\n",
    "\n",
    "# Few-shot示例：問答\n",
    "qa_prompt = \"\"\"Example 1:\n",
    "問題：台北101有偌懸？\n",
    "答案：台北101的高度是五百空八公尺。\n",
    "\n",
    "Example 2:\n",
    "問題：台灣上長的溪仔是佗一條？\n",
    "答案：台灣上長的溪仔是濁水溪，規个長度有百八公里遐爾長。\n",
    "\n",
    "Example 3:\n",
    "問題：臺灣上懸的山是啥物？\n",
    "答案：\"\"\"\n",
    "\n",
    "# Few-shot示例：台語翻譯\n",
    "translation_prompt = \"\"\"Example 1:\n",
    "中文：你好嗎？\n",
    "台語：你好無？\n",
    "\n",
    "Example 2:\n",
    "中文：我很喜歡吃水果。\n",
    "台語：我真愛食水果。\n",
    "\n",
    "Example 3:\n",
    "中文：是否有人會講台語？\n",
    "台語：敢有人會曉講臺語？\n",
    "\n",
    "Example 4:\n",
    "中文：請問這裡怎麼走到火車站？\n",
    "台語：\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': '水果，'}],\n",
       " [{'generated_text': '正面'}],\n",
       " [{'generated_text': '臺灣上懸的山是玉山，伊的懸度是三千九百五十二公尺。'}],\n",
       " [{'generated_text': '請問這欲按怎去到火車站？'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([clm_prompt, sentiment_prompt, qa_prompt, translation_prompt], return_full_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taigi-Llama-2-Translator-series: A Comprehensive Translator for Traditional Chinese, English, and Taigi (POJ, Hanzi and Hanlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c4d06b914440d0834f80d9de428e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = \"Bohanlu/Taigi-Llama-2-Translator-7B\" # or Bohanlu/Taigi-Llama-2-Translator-13B for the 13B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "\n",
    "accelerator = accelerate.Accelerator()\n",
    "pipe = get_pipeline(model_dir, tokenizer, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_sentence='How are you today？'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bohan1234/anaconda3/envs/twllm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bohan1234/anaconda3/envs/twllm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Hanzi: 你今仔日好無？\n",
      "To POJ: Lí kin-á-ji̍t án-chóaⁿ?\n",
      "To Traditional Chinese: 你今天好嗎？\n",
      "To Hanlo: 你今仔日好無？\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"[TRANS]\\n{source_sentence}\\n[/TRANS]\\n[{target_language}]\\n\"\n",
    "def translate(source_sentence:str, target_language:str) -> str:\n",
    "    prompt = PROMPT_TEMPLATE.format(source_sentence=source_sentence, target_language=target_language)\n",
    "    out = pipe(prompt, return_full_text=False, repetition_penalty=1.1, do_sample=False)[0]['generated_text']\n",
    "    return out[:out.find(\"[/\")].strip()\n",
    "\n",
    "source_sentence = \"How are you today？\"\n",
    "\n",
    "print(f\"{source_sentence=}\\n\")\n",
    "print(\"To Hanzi: \" + translate(source_sentence, \"HAN\"))\n",
    "print(\"To POJ: \" + translate(source_sentence, \"POJ\"))\n",
    "print(\"To Traditional Chinese: \" + translate(source_sentence, \"ZH\"))\n",
    "print(\"To Hanlo: \" + translate(source_sentence, \"HL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_sentence='Thài-khong pêng-iú, lín hó! Lín chia̍h-pá--bē?'\n",
      "\n",
      "To Hanzi: 太空朋友，恁好！恁食飽未？\n",
      "To English: Space friends, you guys are great! Have you eaten yet?\n",
      "To Traditional Chinese: 太空朋友，你們好！你們吃飽了嗎？\n",
      "To Hanlo: 太空朋友，lín好！Lín食飽--未？\n"
     ]
    }
   ],
   "source": [
    "source_sentence = \"Thài-khong pêng-iú, lín hó! Lín chia̍h-pá--bē?\"\n",
    "\n",
    "print(f\"{source_sentence=}\\n\")\n",
    "print(\"To Hanzi: \" + translate(source_sentence, \"HAN\"))\n",
    "print(\"To English: \" + translate(source_sentence, \"EN\"))\n",
    "print(\"To Traditional Chinese: \" + translate(source_sentence, \"ZH\"))\n",
    "print(\"To Hanlo: \" + translate(source_sentence, \"HL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taigi-Llama-2-Chat Series: A Comprehensive Chat Model for Taigi (To be completed soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
